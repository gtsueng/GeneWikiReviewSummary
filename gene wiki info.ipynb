{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling info from Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import requests\n",
    "import mwclient\n",
    "from mwclient import Site\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "#useragent = {\n",
    "#    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "#}\n",
    "\n",
    "useragent = {\n",
    "    'User-Agent': 'Gene Wiki Review Impact (gtsueng@scripps.edu)'\n",
    "}\n",
    "\n",
    "site = Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])\n",
    "\n",
    "datapath = 'data/'\n",
    "exppath = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from dateutil import parser\n",
    "from datetime import date\n",
    "import pathlib\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=5,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (site,titlelist):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in titlelist:\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = site.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, titlelist):\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    timestart = datetime.datetime.now().time()\n",
    "    print(timestart,'obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in titlelist:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pandas.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling gene specific infor by Wikipedia titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surfactant_protein_A1\n"
     ]
    }
   ],
   "source": [
    "## Import the urls for the genes\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "#print(gene_wiki_info.head(n=2))\n",
    "pagelist = gene_wiki_info['Gene Wiki Page'].loc[~gene_wiki_info['Gene Wiki Page'].isna()].tolist()\n",
    "titlelist = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in pagelist]\n",
    "print(titlelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining wikipedia volume information\n",
      "                   title  page_length          last_touched   lastrevid\n",
      "0  Surfactant protein A1        31194  2021-11-20T12:42:45Z  1056213617\n",
      "1  Surfactant protein A2        31602  2021-11-19T17:21:15Z  1056087140\n"
     ]
    }
   ],
   "source": [
    "## Get Wikipedia info for gene wiki articles\n",
    "pageinfo,pagefails = get_wiki_volume_info(site,titlelist)\n",
    "wikiinfo = pandas.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:31:27.970202 obtaining wikipedia pageview information\n",
      "                   title  views granularity   timestamp      access agent\n",
      "0  Surfactant_protein_A1    146     monthly  2017090100  all-access  user\n",
      "1  Surfactant_protein_A1    125     monthly  2017100100  all-access  user\n"
     ]
    }
   ],
   "source": [
    "#### Get Page views for each Gene Wiki Review wikipedia entry\n",
    "\n",
    "#pages = [\"Cyclin-dependent kinase 1\", \"Reelin\"] ## for unit test\n",
    "\n",
    "pv_params = {'project':'en.wikipedia',\n",
    "             'access':'all-access/',\n",
    "             'agent':'user/',\n",
    "             'granularity':'monthly/',\n",
    "             'start':'20130101',\n",
    "             'end':'20211115'}\n",
    "\n",
    "gene_monthly_pvs,pgfails = get_monthly_pvs(pv_params,useragent, titlelist)\n",
    "print(gene_monthly_pvs.head(n=2))\n",
    "\n",
    "gene_monthly_pvs.to_csv(exppath+'gene_wiki_views.tsv',sep='\\t',header=True)\n",
    "\n",
    "gene_monthly_views = pandas.pivot_table(gene_monthly_pvs[['timestamp','title','views']],\n",
    "                                        values='views',index='title',columns='timestamp')\n",
    "gene_pvs = gene_monthly_views.reset_index()\n",
    "gene_pvs.rename(columns={'title':'wikipedia'},inplace=True)\n",
    "#print(gene_pvs)\n",
    "gene_pvs.to_csv(exppath+'gw_pvs.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all statements added for series via SPARQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Wikidata for P179 (part of series) of Q108807010 (Gene Wiki Review Series). Then identify statements that use any member of the query results as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the sparql query to retrieve all Articles in this series\n",
    "\n",
    "def fetch_gwreviews_wd():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?itemLabel \n",
    "    WHERE \n",
    "    {\n",
    "      ?item wdt:P179 wd:Q108807010. \n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } # Helps get the label in your language, if not, then en language\n",
    "    }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    cleandata = datadf[['uri','label','QID']].copy()\n",
    "    return(cleandata)\n",
    "\n",
    "\n",
    "def load_props_to_check(DATAPATH):\n",
    "    propinfo = read_csv(os.path.join(DATAPATH,'propertylist.tsv'),delimiter='\\t',header=0)\n",
    "    return(propinfo)\n",
    "\n",
    "\n",
    "def clean_up_results(wdjson,pid):\n",
    "    tmpdf = pd.DataFrame(wdjson['results']['bindings'])\n",
    "    tmpdf['subjectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['item']]\n",
    "    tmpdf['objectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['value']]\n",
    "    tmpdf['subject'] = [x['value'] for x in tmpdf['itemLabel']]\n",
    "    tmpdf['object'] = [x['value'] for x in tmpdf['valueLabel']]\n",
    "    tmpdf['predicatePID'] = pid\n",
    "    cleandf = tmpdf[['subjectQID','predicatePID','objectQID','subject','object']].copy()\n",
    "    return(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run query to retrieve all statements that reference the above articles\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "propinfo = load_props_to_check(DATAPATH)\n",
    "usefulpids = propinfo['Property'].loc[propinfo['PropertyUse']=='main'].unique().tolist()\n",
    "\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "refids = cleandata['QID'].unique().tolist()\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "querybase = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  \"\"\"\n",
    "\n",
    "queryend = \"\"\"    \n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "resultdf = pd.DataFrame(columns = (['subjectQID','predicatePID','objectQID','subject','object']))\n",
    "for refqid in refids:\n",
    "    for pid in usefulpids:\n",
    "        #refqid = 'Q65950306' ## For testing\n",
    "        #pid = 'P1916' ## For testing\n",
    "        refquery = f\"  ?ref ?prop wd:{refqid} .\" \n",
    "        propquery = f\"  ?item wdt:{pid} ?value\"  \n",
    "        query = querybase+refquery+propquery+queryend\n",
    "        try:\n",
    "            r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "            tmpdata = r.json()\n",
    "            if len(tmpdata['results']['bindings']) <= 0:\n",
    "                no_result_flag = True\n",
    "            else:\n",
    "                cleandf = clean_up_results(tmpdata,pid)\n",
    "                resultdf = pd.concat((resultdf,cleandf),ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "resultdf.to_csv('results/wd_statements_added.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                item  \\\n",
      "0  {'type': 'uri', 'value': 'http://www.wikidata....   \n",
      "\n",
      "                                               value  \\\n",
      "0  {'type': 'uri', 'value': 'http://www.wikidata....   \n",
      "\n",
      "                                           itemLabel  \\\n",
      "0  {'xml:lang': 'en', 'type': 'literal', 'value':...   \n",
      "\n",
      "                                          valueLabel  \n",
      "0  {'xml:lang': 'en', 'type': 'literal', 'value':...  \n",
      "  subjectQID predicatePID objectQID subject         object\n",
      "0  Q18033696        P2293    Q41112   KALRN  schizophrenia\n"
     ]
    }
   ],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  ?ref ?prop wd:Q102060922 . # Replace with specific QID of article as a reference under 'stated in' within a statement\n",
    "  ?item wdt:P2293 ?value  # Specify property of statement (in this example, Genetic Assocation)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "test = r.json()\n",
    "print(pd.DataFrame(test['results']['bindings']))\n",
    "\n",
    "\n",
    "cleantest = clean_up_results(test,'P2293')\n",
    "print(cleantest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
