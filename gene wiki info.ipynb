{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling info from Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import mwclient as mw\n",
    "import pywikibot as pwb\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pathlib\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = {\n",
    "    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "}\n",
    "\n",
    "mwsite = mw.Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])\n",
    "\n",
    "datapath = 'data/'\n",
    "exppath = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    "    #allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=5,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, no_missing):\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    print('obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in no_missing['titlelist']:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pd.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (mwsite,no_missing):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in no_missing['titlelist'].tolist():\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = mwsite.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to get revision ids\n",
    "###############################################################################\n",
    "from time import mktime\n",
    "\n",
    "def get_revid(site,pagetitle,starttime):\n",
    "    page = site.pages[pagetitle]\n",
    "    revidlist = []\n",
    "    for revision in page.revisions():\n",
    "        dt = datetime.fromtimestamp(mktime(revision['timestamp']))\n",
    "        if dt <= datetime.strptime(starttime,'%Y%m%d'):\n",
    "            revidlist.append(revision['revid'])\n",
    "    return(revidlist[0])\n",
    "\n",
    "def get_latest_revid(site,pagetitle):\n",
    "    page = site.pages[pagetitle]\n",
    "    allrevisions = list(page.revisions(prop='ids'))  \n",
    "    last_revision_id = allrevisions[-1]['revid']\n",
    "    return(last_revision_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to compare revisions\n",
    "###############################################################################\n",
    "def compare_revisions(mwsite,pagetitle,oldrevid,latestrevid):\n",
    "    compare_result = mwsite.get('compare', fromrev=latestid, torev=oldrevid, fromtitle=pagetitle,\n",
    "                              totitle=pagetitle)\n",
    "    return(compare_result['compare']['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pywikibot to get the text from old versions of wikipedia\n",
    "## pages\n",
    "###############################################################################\n",
    "\n",
    "def get_six_months_prior(adatestring):\n",
    "    dateinfo = datetime.strptime(adatestring, \"%Y%m%d\")\n",
    "    six_months_prior = dateinfo - relativedelta(months = 6)\n",
    "    starttime = datetime.strftime(six_months_prior,\"%Y%m%d\")\n",
    "    return(starttime)\n",
    "\n",
    "\n",
    "def get_old_page_length(pagetitle, oldrevid):\n",
    "    pwsite = pwb.Site(\"en\", \"wikipedia\")\n",
    "    pwpage = pwb.Page(pwsite, pagetitle)\n",
    "    text = pwpage.getOldVersion(oldid = oldrevid)\n",
    "    return(len(text))\n",
    "\n",
    "\n",
    "def get_old_page_volumes(mwsite,no_missing):\n",
    "    print('obtaining old wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    for i in range(len(no_missing)):\n",
    "        pagetitle = no_missing.iloc[i]['titlelist']\n",
    "        updatedate = no_missing.iloc[i]['Wikipedia update period']\n",
    "        starttime = get_six_months_prior(updatedate)\n",
    "        tempdict={'title':pagetitle,'Wikipedia update period':updatedate,'6 months before update':starttime}\n",
    "        try:\n",
    "            oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "            oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "            tempdict['first revision prior to 6 month date'] = oldrevid\n",
    "            tempdict['corresponding length'] = oldpagevolume\n",
    "            pageinfo.append(tempdict)\n",
    "        except:\n",
    "            ## The page did not exist six months prior to the author adding, so page volume prior is 0\n",
    "            tempdict['first revision prior to 6 month date'] = 0\n",
    "            tempdict['corresponding length'] = 0\n",
    "            pageinfo.append(tempdict)               \n",
    "        time.sleep(1)\n",
    "    return(pageinfo)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling gene specific infor by Wikipedia titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surfactant_protein_A1\n"
     ]
    }
   ],
   "source": [
    "## Import the urls for the genes\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "no_missing = gene_wiki_info.loc[~gene_wiki_info['Gene Wiki Page'].isna()].copy()\n",
    "no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(int)\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(str)\n",
    "print(no_missing.iloc[0]['titlelist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  Gene_id Date anticipated Latest email sent (MJ) GW_title  \\\n",
      "0   NaN   653509   July--COMPLETE              7/18/2012   SFTPA1   \n",
      "1   NaN   729238   July--COMPLETE              7/18/2012   SFTPA2   \n",
      "\n",
      "  corresponding author Journal state  \\\n",
      "0               Floros     published   \n",
      "1               Floros     published   \n",
      "\n",
      "                                              status  \\\n",
      "0  http://www.sciencedirect.com/science/article/p...   \n",
      "1  http://www.sciencedirect.com/science/article/p...   \n",
      "\n",
      "                                        Pubmed Gene Wiki Status  \\\n",
      "0  http://www.ncbi.nlm.nih.gov/pubmed/23069847         Complete   \n",
      "1  http://www.ncbi.nlm.nih.gov/pubmed/23069847         Complete   \n",
      "\n",
      "                                      Gene Wiki Page  \\\n",
      "0  https://en.wikipedia.org/wiki/Surfactant_prote...   \n",
      "1  https://en.wikipedia.org/wiki/Surfactant_prote...   \n",
      "\n",
      "                         Notes Acknowledgements Grant ID Editor    PMCID  \\\n",
      "0  Completed along with SFTPA2                       NaN    NaN  3570704   \n",
      "1  Completed along with SFTPA1              NaN      NaN    NaN  3570704   \n",
      "\n",
      "  Wikipedia update period              titlelist  \n",
      "0                20120830  Surfactant_protein_A1  \n",
      "1                20120830  Surfactant_protein_A2  \n"
     ]
    }
   ],
   "source": [
    "print(no_missing.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info = no_missing[['Batch','Gene Wiki Page','status','Wikipedia update period']]\n",
    "basic_info.to_csv('results/basic_info.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining wikipedia volume information\n",
      "                   title  page_length          last_touched   lastrevid\n",
      "0  Surfactant protein A1        31194  2021-12-01T01:46:22Z  1056213617\n",
      "1  Surfactant protein A2        31602  2021-12-07T14:12:19Z  1056087140\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Get Wikipedia info for gene wiki articles\n",
    "pageinfo,pagefails = get_wiki_volume_info(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining old wikipedia volume information\n",
      "                   title Wikipedia update period 6 months before update  \\\n",
      "0  Surfactant_protein_A1                20120830               20120229   \n",
      "1  Surfactant_protein_A2                20120830               20120229   \n",
      "\n",
      "   first revision prior to 6 month date  corresponding length  \n",
      "0                             462319030                  5903  \n",
      "1                             204784710                    21  \n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Get past Wikipedia info for gene wiki articles\n",
    "pageinfo = get_old_page_volumes(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info-BEFORE.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test of functions\n",
    "pagetitle = 'Surfactant_protein_A1'\n",
    "starttime = '20120229'\n",
    "oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "print(oldrevid)\n",
    "oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "print(oldpagevolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining wikipedia pageview information\n",
      "                   title  views granularity   timestamp      access agent\n",
      "0  Surfactant_protein_A1    146     monthly  2017090100  all-access  user\n",
      "1  Surfactant_protein_A1    125     monthly  2017100100  all-access  user\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Get Page views for each Gene Wiki Review wikipedia entry\n",
    "\n",
    "#pages = [\"Cyclin-dependent kinase 1\", \"Reelin\"] ## for unit test\n",
    "\n",
    "pv_params = {'project':'en.wikipedia',\n",
    "             'access':'all-access/',\n",
    "             'agent':'user/',\n",
    "             'granularity':'monthly/',\n",
    "             'start':'20130101',\n",
    "             'end':'20211115'}\n",
    "\n",
    "gene_monthly_pvs,pgfails = get_monthly_pvs(pv_params,useragent, no_missing)\n",
    "print(gene_monthly_pvs.head(n=2))\n",
    "\n",
    "gene_monthly_pvs.to_csv(exppath+'gene_wiki_views.tsv',sep='\\t',header=True)\n",
    "\n",
    "gene_monthly_views = pd.pivot_table(gene_monthly_pvs[['timestamp','title','views']],\n",
    "                                        values='views',index='title',columns='timestamp')\n",
    "gene_pvs = gene_monthly_views.reset_index()\n",
    "gene_pvs.rename(columns={'title':'wikipedia'},inplace=True)\n",
    "#print(gene_pvs)\n",
    "gene_pvs.to_csv(exppath+'gw_pvs.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all statements added for series via SPARQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Wikidata for P179 (part of series) of Q108807010 (Gene Wiki Review Series). Then identify statements that use any member of the query results as a reference. Note that the SPARQL query will return a number of positive results, but it also returns a lot of false positives as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the sparql query to retrieve all Articles in this series\n",
    "\n",
    "def fetch_gwreviews_wd():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?PubMedCentID \n",
    "    WHERE \n",
    "    {\n",
    "      ?item wdt:P179 wd:Q108807010.\n",
    "        ?item wdt:P932 ?PubMedCentID\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } # Helps get the label in your language, if not, then en language\n",
    "    }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['PMCID'] = [x['value'] for x in datadf['PubMedCentID']]\n",
    "    cleandata = datadf[['uri','label','QID','PMCID']].copy()\n",
    "    return(cleandata)\n",
    "\n",
    "\n",
    "def load_props_to_check(DATAPATH):\n",
    "    propinfo = read_csv(os.path.join(DATAPATH,'propertylist.tsv'),delimiter='\\t',header=0)\n",
    "    return(propinfo)\n",
    "\n",
    "\n",
    "def clean_up_results(wdjson,pid):\n",
    "    tmpdf = pd.DataFrame(wdjson['results']['bindings'])\n",
    "    tmpdf['subjectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['item']]\n",
    "    tmpdf['objectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['value']]\n",
    "    tmpdf['subject'] = [x['value'] for x in tmpdf['itemLabel']]\n",
    "    tmpdf['object'] = [x['value'] for x in tmpdf['valueLabel']]\n",
    "    tmpdf['predicatePID'] = pid\n",
    "    try:\n",
    "        tmpdf['qualifierID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['qualifier']]\n",
    "        tmpdf['qualifier'] = [x['value'] for x in tmpdf['qualifierLabel']]\n",
    "    except:\n",
    "        tmpdf['qualifierID'] = \"None\"\n",
    "        tmpdf['qualifier'] = \"None\"\n",
    "    cleandf = tmpdf[['subjectQID','predicatePID','objectQID','subject','object','qualifierID','qualifier']].copy()\n",
    "    return(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=5)\")': /sparql?format=json&query=%0A++++++++SELECT+DISTiNCT+%3Fitem+%3FitemLabel+%3Fvalue+%3FvalueLabel+%3Fqualifier+%3FqualifierLabel%0A++++++++WHERE+%7B%0A++++++++++%3Fitem+%3Fp+%3Fstatement.%0A++++++++++%3Fstatement+prov%3AwasDerivedFrom+%3Fref+.+%0A++++++++++%3Fref+pr%3AP248+wd%3AQ28085339+.%0A++++++++++%3Fstatement+pq%3AP459+%3Fqualifier+.%0A++++++++++%3Fitem+wdt%3AP924+%3Fvalue%0A++++++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22%5BAUTO_LANGUAGE%5D%2Cen%22.+%7D%0A++++++++%7D%0A++++++++\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=5)\")': /sparql?format=json&query=%0A++++++++SELECT+DISTiNCT+%3Fitem+%3FitemLabel+%3Fvalue+%3FvalueLabel+%3Fqualifier+%3FqualifierLabel%0A++++++++WHERE+%7B%0A++++++++++%3Fitem+%3Fp+%3Fstatement.%0A++++++++++%3Fstatement+prov%3AwasDerivedFrom+%3Fref+.+%0A++++++++++%3Fref+pr%3AP248+wd%3AQ28085339+.%0A++++++++++%3Fstatement+pq%3AP459+%3Fqualifier+.%0A++++++++++%3Fitem+wdt%3AP924+%3Fvalue%0A++++++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22%5BAUTO_LANGUAGE%5D%2Cen%22.+%7D%0A++++++++%7D%0A++++++++\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=5)\")': /sparql?format=json&query=%0A++++++++SELECT+DISTiNCT+%3Fitem+%3FitemLabel+%3Fvalue+%3FvalueLabel+%3Fqualifier+%3FqualifierLabel%0A++++++++WHERE+%7B%0A++++++++++%3Fitem+%3Fp+%3Fstatement.%0A++++++++++%3Fstatement+prov%3AwasDerivedFrom+%3Fref+.+%0A++++++++++%3Fref+pr%3AP248+wd%3AQ38779105+.%0A++++++++++%3Fstatement+pq%3AP459+%3Fqualifier+.%0A++++++++++%3Fitem+wdt%3AP780+%3Fvalue%0A++++++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22%5BAUTO_LANGUAGE%5D%2Cen%22.+%7D%0A++++++++%7D%0A++++++++\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=5)\")': /sparql?format=json&query=%0A++++++++SELECT+DISTiNCT+%3Fitem+%3FitemLabel+%3Fvalue+%3FvalueLabel+%3Fqualifier+%3FqualifierLabel%0A++++++++WHERE+%7B%0A++++++++++%3Fitem+%3Fp+%3Fstatement.%0A++++++++++%3Fstatement+prov%3AwasDerivedFrom+%3Fref+.+%0A++++++++++%3Fref+pr%3AP248+wd%3AQ38779105+.%0A++++++++++%3Fstatement+pq%3AP459+%3Fqualifier+.%0A++++++++++%3Fitem+wdt%3AP924+%3Fvalue%0A++++++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22%5BAUTO_LANGUAGE%5D%2Cen%22.+%7D%0A++++++++%7D%0A++++++++\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='query.wikidata.org', port=443): Read timed out. (read timeout=5)\")': /sparql?format=json&query=%0A++++++++SELECT+DISTiNCT+%3Fitem+%3FitemLabel+%3Fvalue+%3FvalueLabel+%3Fqualifier+%3FqualifierLabel%0A++++++++WHERE+%7B%0A++++++++++%3Fitem+%3Fp+%3Fstatement.%0A++++++++++%3Fstatement+prov%3AwasDerivedFrom+%3Fref+.+%0A++++++++++%3Fref+pr%3AP248+wd%3AQ38779105+.%0A++++++++++%3Fstatement+pq%3AP459+%3Fqualifier+.%0A++++++++++%3Fitem+wdt%3AP1912+%3Fvalue%0A++++++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22%5BAUTO_LANGUAGE%5D%2Cen%22.+%7D%0A++++++++%7D%0A++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subjectQID predicatePID objectQID      subject       object qualifierID  \\\n",
      "0     Q57055         P769   Q423364  paracetamol  propranolol   Q23173789   \n",
      "1     Q57055         P769  Q1135705  paracetamol    rifabutin   Q23173789   \n",
      "\n",
      "  qualifier  \n",
      "0       EXP  \n",
      "1       EXP  \n",
      "Wall time: 2h 50min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### With Qualifiers\n",
    "## Run query to retrieve all statements that reference the above articles\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "propinfo = load_props_to_check(DATAPATH)\n",
    "usefulpids = propinfo['Property'].loc[propinfo['PropertyUse']=='main'].unique().tolist()\n",
    "\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "refids = cleandata['QID'].unique().tolist()\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "resultdf = pd.DataFrame(columns = (['subjectQID','predicatePID','objectQID','subject','object','qualifierID','qualifier']))\n",
    "for refqid in refids:\n",
    "    for pid in usefulpids:\n",
    "        #refqid = 'Q65950306' ## For testing\n",
    "        #pid = 'P1916' ## For testing\n",
    "        querybase = f\"\"\"\n",
    "        SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel ?qualifier ?qualifierLabel\n",
    "        WHERE {{\n",
    "          ?item ?p ?statement.\n",
    "          ?statement prov:wasDerivedFrom ?ref . \n",
    "          ?ref pr:P248 wd:{refqid} .\n",
    "          ?statement pq:P459 ?qualifier .\n",
    "          ?item wdt:{pid} ?value\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            r = httprequests.get(url, params = {'format': 'json', 'query': querybase})\n",
    "            tmpdata = r.json()\n",
    "            if len(tmpdata['results']['bindings']) <= 0:\n",
    "                no_result_flag = True\n",
    "            else:\n",
    "                cleandf = clean_up_results(tmpdata,pid)\n",
    "                resultdf = pd.concat((resultdf,cleandf),ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "print(resultdf.head(n=2))\n",
    "resultdf.to_csv('results/wd_statements_added.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach\n",
    "The above sparql query is giving a lot of false positives. To bypass, try simplifying the query to pull all Wikidata items that have statements which reference a Gene Wiki Review. Request the entire content for each item from the MediaWiki API (https://www.wikidata.org/w/api.php?action=wbgetclaims&entity=Q108&format=json) then parse it to extract only the statements with the correct references. Note that these may be very nested; however, this may be more comprehensive that trying to query for specific properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subjectQID       subject\n",
      "0    Q155746  paricalcitol\n",
      "1    Q415571     Bleomycin\n",
      "37\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run query to retrieve all items statements that reference the above articles\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "refids = cleandata['QID'].unique().tolist()\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "resultdf = pd.DataFrame(columns = (['subjectQID','subject']))\n",
    "for refqid in refids:\n",
    "    querybase = f\"\"\"\n",
    "    SELECT DISTiNCT ?item ?itemLabel\n",
    "    WHERE {{\n",
    "      ?item ?p ?statement.\n",
    "      ?statement prov:wasDerivedFrom [pr:P248 wd:{refqid}] . \n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"        \n",
    "    try:\n",
    "        r = httprequests.get(url, params = {'format': 'json', 'query': querybase})\n",
    "        tmpdata = r.json()\n",
    "        if len(tmpdata['results']['bindings']) <= 0:\n",
    "            no_result_flag = True\n",
    "        else:\n",
    "            tmpdf = pd.DataFrame(tmpdata['results']['bindings'])\n",
    "            tmpdf['subjectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['item']]\n",
    "            tmpdf['subject'] = [x['value'] for x in tmpdf['itemLabel']]\n",
    "            resultdf = pd.concat((resultdf,tmpdf[['subjectQID','subject']].copy()),ignore_index=True)\n",
    "    except:\n",
    "        continue\n",
    "    time.sleep(2)\n",
    "\n",
    "print(resultdf.head(n=2))\n",
    "print(len(resultdf['subjectQID'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_df(jsonresult):\n",
    "    testdf = pd.DataFrame(jsonresult)\n",
    "    testdf.reset_index(inplace=True)\n",
    "    testdf.rename(columns={'index':'property'},inplace=True)\n",
    "    tmpdf = testdf.explode('claims')\n",
    "    newtmpdf = pd.DataFrame(tmpdf['claims'].tolist())\n",
    "    refdf = newtmpdf.explode('references')\n",
    "    cleandf = refdf.loc[~refdf['references'].isna()]\n",
    "    return(cleandf)\n",
    "\n",
    "def extract_property(amainsnak):\n",
    "    propID = amainsnak['property']\n",
    "    return(propID)\n",
    "\n",
    "def extract_object(amainsnak):\n",
    "    try:\n",
    "        datavaluedict = amainsnak['datavalue']['value']\n",
    "        if isinstance(datavaluedict, dict):\n",
    "            try:\n",
    "                objectID = datavaluedict['id']\n",
    "            except:\n",
    "                objectID = False\n",
    "        else:\n",
    "            objectID = False\n",
    "    except:\n",
    "        objectID = False\n",
    "    return(objectID)\n",
    "\n",
    "def parserefID(refvalue):\n",
    "    if isinstance(refvalue, dict):\n",
    "        try:\n",
    "            referenceID = refvalue['id']\n",
    "        except:\n",
    "            referenceID = False\n",
    "    else:\n",
    "        referenceID = False\n",
    "    return(referenceID)\n",
    "\n",
    "def extract_references(areference):\n",
    "    referencelist = []\n",
    "    try:\n",
    "        refsnak = areference['snaks']\n",
    "    except:\n",
    "        refsnak = areference\n",
    "    if isinstance(refsnak, dict):\n",
    "        for key in refsnak.keys():\n",
    "            value = refsnak[key]\n",
    "            try:\n",
    "                if isinstance(value, dict):\n",
    "                    refprop = value['property']\n",
    "                    refvalue = value['datavalue']['value']\n",
    "                    referenceID = parserefID(refvalue)\n",
    "                    if ((referenceID != False) and (referenceID != None) and (referenceID != \"\")):\n",
    "                        referencelist.append({'refproperty':refprop,'refID':referenceID})\n",
    "                elif isinstance(value,list):\n",
    "                    for i in range(len(value)):\n",
    "                        refprop = value[i]['property']\n",
    "                        refvalue = value[i]['datavalue']['value']\n",
    "                        referenceID = parserefID(refvalue)\n",
    "                        if ((referenceID != False) and (referenceID != None) and (referenceID != \"\")):\n",
    "                            referencelist.append({'refproperty':refprop,'refID':referenceID})                \n",
    "                else:\n",
    "                    refprop = False\n",
    "                    referenceID = False\n",
    "            except:\n",
    "                continue\n",
    "    return(referencelist)\n",
    "\n",
    "#### functions to rip out reference property and refid since json_normalize library failed\n",
    "def refprop2df(refdict):\n",
    "    if isinstance(refdict, dict):\n",
    "        try:\n",
    "            refprop = refdict['refproperty']\n",
    "        except:\n",
    "            refprop = False\n",
    "    else:\n",
    "        refprop = False\n",
    "    return(refprop)\n",
    "\n",
    "def refid2df(refdict):\n",
    "    if isinstance(refdict, dict):\n",
    "        try:\n",
    "            refid = refdict['refID']\n",
    "        except:\n",
    "            refid = False\n",
    "    else:\n",
    "        refid = False\n",
    "    return(refid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wd_json(qjson):\n",
    "    newdf = convert_json_to_df(qjson)\n",
    "    newdf['propID']=newdf.apply(lambda row: extract_property(row['mainsnak']), axis = 1)\n",
    "    newdf['objectID'] = newdf.apply(lambda row: extract_object(row['mainsnak']), axis=1)\n",
    "    newdf['reflist'] = newdf.apply(lambda row: extract_references(row['references']), axis=1)\n",
    "    refdf = newdf.explode('reflist').copy()\n",
    "    refdf['refprop'] = refdf.apply(lambda row: refprop2df(row['reflist']), axis=1)\n",
    "    refdf['refID'] = refdf.apply(lambda row: refid2df(row['reflist']), axis=1)\n",
    "    refdf['qualist'] = refdf.apply(lambda row: extract_references(row['qualifiers']), axis=1)\n",
    "    qualdf = refdf.explode('qualist').copy()\n",
    "    qualdf['qualprop'] = qualdf.apply(lambda row: refprop2df(row['qualist']), axis=1)\n",
    "    qualdf['qualID'] = qualdf.apply(lambda row: refid2df(row['qualist']), axis=1)\n",
    "    cleandf = qualdf[['propID','objectID','qualprop','qualID','refprop','refID']].copy()\n",
    "    return(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P1013': [{'snaktype': 'value', 'property': 'P1013', 'hash': '4262181141504a534032aed1a9cdd8ab0b806f96', 'datavalue': {'value': {'entity-type': 'item', 'numeric-id': 107139329, 'id': 'Q107139329'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item'}], 'P7469': [{'snaktype': 'value', 'property': 'P7469', 'hash': '3b611021cfc3d6d93851621cc12346cf9df0bf77', 'datavalue': {'value': {'entity-type': 'item', 'numeric-id': 20749305, 'id': 'Q20749305'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item'}]}\n"
     ]
    }
   ],
   "source": [
    "tmpdf = convert_json_to_df(qjson)\n",
    "print(tmpdf.iloc[0]['qualifiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entitylist = resultdf['subjectQID'].unique().tolist()\n",
    "\n",
    "wikidatadf = pd.DataFrame(columns=['subjectID','propID','objectID','qualprop','qualID','refprop','refID'])\n",
    "for eachentity in entitylist:\n",
    "    r = httprequests.get(f'https://www.wikidata.org/w/api.php?action=wbgetclaims&entity={eachentity}&format=json')\n",
    "    qjson = r.json()\n",
    "    tmpdf = parse_wd_json(qjson)\n",
    "    tmpdf['subjectID'] = eachentity\n",
    "    wikidatadf = pd.concat((wikidatadf,tmpdf),ignore_index=True)\n",
    "    time.sleep(1)\n",
    "    #print(eachentity, len(tmpdf), len(wikidatadf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "   subjectID propID   objectID qualprop     qualID refprop      refID\n",
      "46   Q155746  P3771  Q14906849     P459  Q23173789    P248  Q65950306\n",
      "99   Q415571  P3771  Q24419440     P459  Q23173789    P248  Q65950306\n"
     ]
    }
   ],
   "source": [
    "gw_wikidata = wikidatadf.loc[wikidatadf['refID'].isin(refids)]\n",
    "print(len(gw_wikidata))\n",
    "print(gw_wikidata.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Citation Metrics for All Gene Wiki Reviews\n",
    "There is a SERP API for accessing information from google scholar. This API can potentially be used to pull the number of citations garnered by each Gene Wiki Review article.\n",
    "\n",
    "Example API Call - https://serpapi.com/playground?engine=google_scholar&q=Gene+Wiki+Reviews-Raising+the+quality+and+accessibility+of+information+about+the+human+genome&hl=en\n",
    "\n",
    "**Note that it appears the results for the API call are not machine-readable as it's a playground API meant for user exploration. Since it's unclear whether or not the Gene Wiki Review editorial will constitute a commercial use, we'l just manually check google scholar and pull the information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KCNE2 K+ channel regulatory subunit: Ubiquitous influence, complex pathobiology\n"
     ]
    }
   ],
   "source": [
    "## Test the use of ther SERPAPI\n",
    "## As evidenced by the results, the API playground is not meant for any real use\n",
    "baseapiurl = 'https://serpapi.com/playground?engine=google_scholar&q='\n",
    "apicallend = '&hl=en'\n",
    "title = cleandata.iloc[0]['label'].replace(\" \",\"+\")\n",
    "testurl = f\"{baseapiurl}{title}{apicallend}\"\n",
    "results = httprequests.get(testurl)\n",
    "print(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              status  \\\n",
      "0  http://www.sciencedirect.com/science/article/p...   \n",
      "2  http://www.sciencedirect.com/science/article/p...   \n",
      "\n",
      "                                        Pubmed            PMCID  \n",
      "0  http://www.ncbi.nlm.nih.gov/pubmed/23069847          3570704  \n",
      "2  http://www.ncbi.nlm.nih.gov/pubmed/23246696  NIHMS ID 909445  \n"
     ]
    }
   ],
   "source": [
    "#### Step 1, pull all gw reviews from the file\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "#print(gene_wiki_info)\n",
    "publishlist = gene_wiki_info['status'].unique().tolist()\n",
    "pubsinfo = gene_wiki_info[['status','Pubmed','PMCID']].loc[gene_wiki_info['status'].isin(publishlist)].copy()\n",
    "pubsinfo.drop_duplicates(inplace=True,keep='first')\n",
    "pubsinfo['PMCID']=pubsinfo['PMCID'].astype(str)\n",
    "print(pubsinfo.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        uri  \\\n",
      "0  http://www.wikidata.org/entity/Q21710689   \n",
      "1  http://www.wikidata.org/entity/Q21710694   \n",
      "\n",
      "                                               label        QID    PMCID  \n",
      "0  The KCNE2 K+ channel regulatory subunit: Ubiqu...  Q21710689  4917011  \n",
      "1  Structural and functional biology of arachidon...  Q21710694  6728142  \n"
     ]
    }
   ],
   "source": [
    "#### Step 2, pull all Gene Wiki Reviews from Wikidata and their Titles (ie- the label)\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "cleandata['PMCID'] = cleandata['PMCID'].astype(str)\n",
    "print(cleandata.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               status  \\\n",
      "95  https://www.sciencedirect.com/science/article/...   \n",
      "96                                                NaN   \n",
      "\n",
      "                                       Pubmed    PMCID  \\\n",
      "95  https://pubmed.ncbi.nlm.nih.gov/34252531/  8318780   \n",
      "96                                        NaN  6660134   \n",
      "\n",
      "                                          uri  \\\n",
      "95  http://www.wikidata.org/entity/Q108806643   \n",
      "96   http://www.wikidata.org/entity/Q38584470   \n",
      "\n",
      "                                                label         QID  \n",
      "95  A role for zinc transporter gene SLC39A12 in t...  Q108806643  \n",
      "96  Cardiac myosin-binding protein C (MYBPC3) in c...   Q38584470  \n"
     ]
    }
   ],
   "source": [
    "#### Step 3- Merge the tables from Step 1 and Step 2\n",
    "allpubinfo = pubsinfo.merge(cleandata, on='PMCID',how = 'outer')\n",
    "print(allpubinfo.tail(n=2))\n",
    "#allpubinfo.to_csv(os.path.join(exppath,'article_citations.tsv'),sep='\\t',header=True)\n",
    "## Note, citations will be pulled manually, so this file exists only to make manual pulls easier to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
