{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling info from Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import mwclient as mw\n",
    "import pywikibot as pwb\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pathlib\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = {\n",
    "    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "}\n",
    "\n",
    "mwsite = mw.Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])\n",
    "\n",
    "datapath = 'data/'\n",
    "exppath = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"] ## Note this method is deprecated and replaced with `allowed_methods` for newer releases of requests library\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=5,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, no_missing):\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    print('obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in no_missing['titlelist']:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pd.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (mwsite,no_missing):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in no_missing['titlelist'].tolist():\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = mwsite.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to get revision ids\n",
    "###############################################################################\n",
    "from time import mktime\n",
    "\n",
    "def get_revid(site,pagetitle,starttime):\n",
    "    page = site.pages[pagetitle]\n",
    "    revidlist = []\n",
    "    for revision in page.revisions():\n",
    "        dt = datetime.fromtimestamp(mktime(revision['timestamp']))\n",
    "        if dt <= datetime.strptime(starttime,'%Y%m%d'):\n",
    "            revidlist.append(revision['revid'])\n",
    "    return(revidlist[0])\n",
    "\n",
    "def get_latest_revid(site,pagetitle):\n",
    "    page = site.pages[pagetitle]\n",
    "    allrevisions = list(page.revisions(prop='ids'))  \n",
    "    last_revision_id = allrevisions[-1]['revid']\n",
    "    return(last_revision_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to compare revisions\n",
    "###############################################################################\n",
    "def compare_revisions(mwsite,pagetitle,oldrevid,latestrevid):\n",
    "    compare_result = mwsite.get('compare', fromrev=latestid, torev=oldrevid, fromtitle=pagetitle,\n",
    "                              totitle=pagetitle)\n",
    "    return(compare_result['compare']['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pywikibot to get the text from old versions of wikipedia\n",
    "## pages\n",
    "###############################################################################\n",
    "\n",
    "def get_six_months_prior(adatestring):\n",
    "    dateinfo = datetime.strptime(adatestring, \"%Y%m%d\")\n",
    "    six_months_prior = dateinfo - relativedelta(months = 6)\n",
    "    starttime = datetime.strftime(six_months_prior,\"%Y%m%d\")\n",
    "    return(starttime)\n",
    "\n",
    "\n",
    "def get_old_page_length(pagetitle, oldrevid):\n",
    "    pwsite = pwb.Site(\"en\", \"wikipedia\")\n",
    "    pwpage = pwb.Page(pwsite, pagetitle)\n",
    "    text = pwpage.getOldVersion(oldid = oldrevid)\n",
    "    return(len(text))\n",
    "\n",
    "\n",
    "def get_old_page_volumes(mwsite,no_missing):\n",
    "    print('obtaining old wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    for i in range(len(no_missing)):\n",
    "        pagetitle = no_missing.iloc[i]['titlelist']\n",
    "        updatedate = no_missing.iloc[i]['Wikipedia update period']\n",
    "        starttime = get_six_months_prior(updatedate)\n",
    "        tempdict={'title':pagetitle,'Wikipedia update period':updatedate,'6 months before update':starttime}\n",
    "        try:\n",
    "            oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "            oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "            tempdict['first revision prior to 6 month date'] = oldrevid\n",
    "            tempdict['corresponding length'] = oldpagevolume\n",
    "            pageinfo.append(tempdict)\n",
    "        except:\n",
    "            ## The page did not exist six months prior to the author adding, so page volume prior is 0\n",
    "            tempdict['first revision prior to 6 month date'] = 0\n",
    "            tempdict['corresponding length'] = 0\n",
    "            pageinfo.append(tempdict)               \n",
    "        time.sleep(1)\n",
    "    return(pageinfo)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling gene specific infor by Wikipedia titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the urls for the genes\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "no_missing = gene_wiki_info.loc[~gene_wiki_info['Gene Wiki Page'].isna()].copy()\n",
    "no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(int)\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(str)\n",
    "print(no_missing.iloc[0]['titlelist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Get Wikipedia info for gene wiki articles\n",
    "pageinfo,pagefails = get_wiki_volume_info(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Get past Wikipedia info for gene wiki articles\n",
    "pageinfo = get_old_page_volumes(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info-BEFORE.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test of functions\n",
    "pagetitle = 'Surfactant_protein_A1'\n",
    "starttime = '20120229'\n",
    "oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "print(oldrevid)\n",
    "oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "print(oldpagevolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Get Page views for each Gene Wiki Review wikipedia entry\n",
    "\n",
    "#pages = [\"Cyclin-dependent kinase 1\", \"Reelin\"] ## for unit test\n",
    "\n",
    "pv_params = {'project':'en.wikipedia',\n",
    "             'access':'all-access/',\n",
    "             'agent':'user/',\n",
    "             'granularity':'monthly/',\n",
    "             'start':'20130101',\n",
    "             'end':'20211115'}\n",
    "\n",
    "gene_monthly_pvs,pgfails = get_monthly_pvs(pv_params,useragent, no_missing)\n",
    "print(gene_monthly_pvs.head(n=2))\n",
    "\n",
    "gene_monthly_pvs.to_csv(exppath+'gene_wiki_views.tsv',sep='\\t',header=True)\n",
    "\n",
    "gene_monthly_views = pd.pivot_table(gene_monthly_pvs[['timestamp','title','views']],\n",
    "                                        values='views',index='title',columns='timestamp')\n",
    "gene_pvs = gene_monthly_views.reset_index()\n",
    "gene_pvs.rename(columns={'title':'wikipedia'},inplace=True)\n",
    "#print(gene_pvs)\n",
    "gene_pvs.to_csv(exppath+'gw_pvs.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all statements added for series via SPARQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Wikidata for P179 (part of series) of Q108807010 (Gene Wiki Review Series). Then identify statements that use any member of the query results as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the sparql query to retrieve all Articles in this series\n",
    "\n",
    "def fetch_gwreviews_wd():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?PubMedCentID \n",
    "    WHERE \n",
    "    {\n",
    "      ?item wdt:P179 wd:Q108807010.\n",
    "        ?item wdt:P932 ?PubMedCentID\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } # Helps get the label in your language, if not, then en language\n",
    "    }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    datadf['PMCID'] = [x['value'] for x in datadf['PubMedCentID']]\n",
    "    cleandata = datadf[['uri','label','QID','PMCID']].copy()\n",
    "    return(cleandata)\n",
    "\n",
    "\n",
    "def load_props_to_check(DATAPATH):\n",
    "    propinfo = read_csv(os.path.join(DATAPATH,'propertylist.tsv'),delimiter='\\t',header=0)\n",
    "    return(propinfo)\n",
    "\n",
    "\n",
    "def clean_up_results(wdjson,pid):\n",
    "    tmpdf = pd.DataFrame(wdjson['results']['bindings'])\n",
    "    tmpdf['subjectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['item']]\n",
    "    tmpdf['objectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['value']]\n",
    "    tmpdf['subject'] = [x['value'] for x in tmpdf['itemLabel']]\n",
    "    tmpdf['object'] = [x['value'] for x in tmpdf['valueLabel']]\n",
    "    tmpdf['predicatePID'] = pid\n",
    "    cleandf = tmpdf[['subjectQID','predicatePID','objectQID','subject','object']].copy()\n",
    "    return(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 29min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run query to retrieve all statements that reference the above articles\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "propinfo = load_props_to_check(DATAPATH)\n",
    "usefulpids = propinfo['Property'].loc[propinfo['PropertyUse']=='main'].unique().tolist()\n",
    "\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "refids = cleandata['QID'].unique().tolist()\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "querybase = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  \"\"\"\n",
    "\n",
    "queryend = \"\"\"    \n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "resultdf = pd.DataFrame(columns = (['subjectQID','predicatePID','objectQID','subject','object']))\n",
    "for refqid in refids:\n",
    "    for pid in usefulpids:\n",
    "        #refqid = 'Q65950306' ## For testing\n",
    "        #pid = 'P1916' ## For testing\n",
    "        refquery = f\"  ?ref ?prop wd:{refqid} .\" \n",
    "        propquery = f\"  ?item wdt:{pid} ?value\"  \n",
    "        query = querybase+refquery+propquery+queryend\n",
    "        try:\n",
    "            r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "            tmpdata = r.json()\n",
    "            if len(tmpdata['results']['bindings']) <= 0:\n",
    "                no_result_flag = True\n",
    "            else:\n",
    "                cleandf = clean_up_results(tmpdata,pid)\n",
    "                resultdf = pd.concat((resultdf,cleandf),ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "resultdf.to_csv('results/wd_statements_added.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a test\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  ?ref ?prop wd:Q102060922 . # Replace with specific QID of article as a reference under 'stated in' within a statement\n",
    "  ?item wdt:P2293 ?value  # Specify property of statement (in this example, Genetic Assocation)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "test = r.json()\n",
    "print(pd.DataFrame(test['results']['bindings']))\n",
    "\n",
    "\n",
    "cleantest = clean_up_results(test,'P2293')\n",
    "print(cleantest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull all statements, chain qualifiers to get all qualifiers of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subjectQID predicatePID objectQID      subject        object\n",
      "0     Q57055         P769   Q410412  paracetamol  carbamazepin\n",
      "1     Q57055         P769   Q177094  paracetamol      imatinib\n"
     ]
    }
   ],
   "source": [
    "statementlist = read_csv(os.path.join(exppath,'wd_statements_added.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(statementlist.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Citation Metrics for All Gene Wiki Reviews\n",
    "There is a SERP API for accessing information from google scholar. This API can potentially be used to pull the number of citations garnered by each Gene Wiki Review article.\n",
    "\n",
    "Example API Call - https://serpapi.com/playground?engine=google_scholar&q=Gene+Wiki+Reviews-Raising+the+quality+and+accessibility+of+information+about+the+human+genome&hl=en\n",
    "\n",
    "**Note that it appears the results for the API call are not machine-readable as it's a playground API meant for user exploration. Since it's unclear whether or not the Gene Wiki Review editorial will constitute a commercial use, we'l just manually check google scholar and pull the information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KCNE2 K+ channel regulatory subunit: Ubiquitous influence, complex pathobiology\n"
     ]
    }
   ],
   "source": [
    "## Test the use of ther SERPAPI\n",
    "## As evidenced by the results, the API playground is not meant for any real use\n",
    "baseapiurl = 'https://serpapi.com/playground?engine=google_scholar&q='\n",
    "apicallend = '&hl=en'\n",
    "title = cleandata.iloc[0]['label'].replace(\" \",\"+\")\n",
    "testurl = f\"{baseapiurl}{title}{apicallend}\"\n",
    "results = httprequests.get(testurl)\n",
    "print(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              status  \\\n",
      "0  http://www.sciencedirect.com/science/article/p...   \n",
      "2  http://www.sciencedirect.com/science/article/p...   \n",
      "\n",
      "                                        Pubmed            PMCID  \n",
      "0  http://www.ncbi.nlm.nih.gov/pubmed/23069847          3570704  \n",
      "2  http://www.ncbi.nlm.nih.gov/pubmed/23246696  NIHMS ID 909445  \n"
     ]
    }
   ],
   "source": [
    "#### Step 1, pull all gw reviews from the file\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "#print(gene_wiki_info)\n",
    "publishlist = gene_wiki_info['status'].unique().tolist()\n",
    "pubsinfo = gene_wiki_info[['status','Pubmed','PMCID']].loc[gene_wiki_info['status'].isin(publishlist)].copy()\n",
    "pubsinfo.drop_duplicates(inplace=True,keep='first')\n",
    "pubsinfo['PMCID']=pubsinfo['PMCID'].astype(str)\n",
    "print(pubsinfo.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        uri  \\\n",
      "0  http://www.wikidata.org/entity/Q21710689   \n",
      "1  http://www.wikidata.org/entity/Q21710694   \n",
      "\n",
      "                                               label        QID    PMCID  \n",
      "0  The KCNE2 K+ channel regulatory subunit: Ubiqu...  Q21710689  4917011  \n",
      "1  Structural and functional biology of arachidon...  Q21710694  6728142  \n"
     ]
    }
   ],
   "source": [
    "#### Step 2, pull all Gene Wiki Reviews from Wikidata and their Titles (ie- the label)\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "cleandata['PMCID'] = cleandata['PMCID'].astype(str)\n",
    "print(cleandata.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               status  \\\n",
      "95  https://www.sciencedirect.com/science/article/...   \n",
      "96                                                NaN   \n",
      "\n",
      "                                       Pubmed    PMCID  \\\n",
      "95  https://pubmed.ncbi.nlm.nih.gov/34252531/  8318780   \n",
      "96                                        NaN  6660134   \n",
      "\n",
      "                                          uri  \\\n",
      "95  http://www.wikidata.org/entity/Q108806643   \n",
      "96   http://www.wikidata.org/entity/Q38584470   \n",
      "\n",
      "                                                label         QID  \n",
      "95  A role for zinc transporter gene SLC39A12 in t...  Q108806643  \n",
      "96  Cardiac myosin-binding protein C (MYBPC3) in c...   Q38584470  \n"
     ]
    }
   ],
   "source": [
    "#### Step 3- Merge the tables from Step 1 and Step 2\n",
    "allpubinfo = pubsinfo.merge(cleandata, on='PMCID',how = 'outer')\n",
    "print(allpubinfo.tail(n=2))\n",
    "#allpubinfo.to_csv(os.path.join(exppath,'article_citations.tsv'),sep='\\t',header=True)\n",
    "## Note, citations will be pulled manually, so this file exists only to make manual pulls easier to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
