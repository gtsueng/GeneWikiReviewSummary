{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling info from Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import mwclient as mw\n",
    "import pywikibot as pwb\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pathlib\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = {\n",
    "    'User-Agent': 'Gene Wiki Review Impact (youremail@domain)'\n",
    "}\n",
    "\n",
    "mwsite = mw.Site('en.wikipedia.org', clients_useragent=useragent['User-Agent'])\n",
    "\n",
    "datapath = 'data/'\n",
    "exppath = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Request nicely\n",
    "###############################################################################\n",
    "\n",
    "DEFAULT_TIMEOUT = 5 # seconds\n",
    "\n",
    "class TimeoutHTTPAdapter(HTTPAdapter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.timeout = DEFAULT_TIMEOUT\n",
    "        if \"timeout\" in kwargs:\n",
    "            self.timeout = kwargs[\"timeout\"]\n",
    "            del kwargs[\"timeout\"]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def send(self, request, **kwargs):\n",
    "        timeout = kwargs.get(\"timeout\")\n",
    "        if timeout is None:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "        return super().send(request, **kwargs)\n",
    "\n",
    "## Set time outs, backoff, retries\n",
    "httprequests = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = TimeoutHTTPAdapter(timeout=5,max_retries=retry_strategy)\n",
    "httprequests.mount(\"https://\", adapter)\n",
    "httprequests.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pulls pageview data from the Media Wiki PageViews API\n",
    "## More on the API here: https://wikimedia.org/api/rest_v1/#/Pageviews%20data/\n",
    "## The module pulls in a parameter dictionary, and the list of wiki titles\n",
    "## Parameters include:\n",
    "## project: en.wikipedia.org, other wikimedia projects\n",
    "## access: all-access, desktop, mobile-app, mobile-web\n",
    "## agent: all-agents, user, spider, bot\n",
    "## granularity: daily, monthly\n",
    "###############################################################################\n",
    "def get_monthly_pvs(page_view_parameters, useragent, no_missing):\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pginfo = []\n",
    "    pgfails = []\n",
    "    print('obtaining wikipedia pageview information')\n",
    "    pv_api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/\"\n",
    "    for eachtitle in no_missing['titlelist']:\n",
    "        try:\n",
    "            url = pv_api_url+pv_params['access']+pv_params['agent']+eachtitle+\"/\"+pv_params['granularity']+pv_params['start']+\"/\"+pv_params['end']\n",
    "            r = httprequests.get(url, headers=useragent)\n",
    "            items = r.json()\n",
    "            try:\n",
    "                for item in items[\"items\"]:\n",
    "                    tmpdict = {'title':item[\"article\"], 'views':int(item[\"views\"]), 'granularity':item['granularity'],\n",
    "                               'timestamp':item[\"timestamp\"],'access':item['access'],'agent':item['agent']}\n",
    "                    pginfo.append(tmpdict)\n",
    "            except:\n",
    "                tmpdict = {'title':title, 'views':-1, 'granularity':\"no data\",\n",
    "                               'timestamp':\"00000000\",'access':\"not data\",'agent':\"no data\"}\n",
    "                pginfo.append(tmpdict)            \n",
    "        except:\n",
    "            pgfails.append(eachtitle)\n",
    "        time.sleep(1)\n",
    "\n",
    "    pginfodf = pd.DataFrame(pginfo)\n",
    "    \n",
    "    return(pginfodf, pgfails)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to pull page size and edit stats on wikipedia pages  \n",
    "## for each gene given a list of gene wikipedia titles\n",
    "###############################################################################\n",
    "def get_wiki_volume_info (mwsite,no_missing):\n",
    "    print('obtaining wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    pagefails = []\n",
    "    for eachpage in no_missing['titlelist'].tolist():\n",
    "        tempdict={} #title, length/size, last_revised, last_revision_id\n",
    "        try:\n",
    "            checkitem = mwsite.api('query', prop='info', titles=eachpage)\n",
    "            results1 = checkitem['query']['pages']\n",
    "            for item in results1:\n",
    "                base = str(item)\n",
    "                results2 = results1[base]\n",
    "                tempdict['title']=str(results2['title'])\n",
    "                tempdict['page_length']=int(results2['length'])\n",
    "                tempdict['last_touched']=str(results2['touched'])\n",
    "                tempdict['lastrevid']=str(results2['lastrevid'])\n",
    "                pageinfo.append(tempdict)               \n",
    "        except:\n",
    "            pagefails.append(eachpage)\n",
    "            pass \n",
    "        time.sleep(1)\n",
    "    return(pageinfo,pagefails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to get revision ids\n",
    "###############################################################################\n",
    "from time import mktime\n",
    "\n",
    "def get_revid(site,pagetitle,starttime):\n",
    "    page = site.pages[pagetitle]\n",
    "    revidlist = []\n",
    "    for revision in page.revisions():\n",
    "        dt = datetime.fromtimestamp(mktime(revision['timestamp']))\n",
    "        if dt <= datetime.strptime(starttime,'%Y%m%d'):\n",
    "            revidlist.append(revision['revid'])\n",
    "    return(revidlist[0])\n",
    "\n",
    "def get_latest_revid(site,pagetitle):\n",
    "    page = site.pages[pagetitle]\n",
    "    allrevisions = list(page.revisions(prop='ids'))  \n",
    "    last_revision_id = allrevisions[-1]['revid']\n",
    "    return(last_revision_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses mwclient to compare revisions\n",
    "###############################################################################\n",
    "def compare_revisions(mwsite,pagetitle,oldrevid,latestrevid):\n",
    "    compare_result = mwsite.get('compare', fromrev=latestid, torev=oldrevid, fromtitle=pagetitle,\n",
    "                              totitle=pagetitle)\n",
    "    return(compare_result['compare']['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## This module uses pywikibot to get the text from old versions of wikipedia\n",
    "## pages\n",
    "###############################################################################\n",
    "\n",
    "def get_six_months_prior(adatestring):\n",
    "    dateinfo = datetime.strptime(adatestring, \"%Y%m%d\")\n",
    "    six_months_prior = dateinfo - relativedelta(months = 6)\n",
    "    starttime = datetime.strftime(six_months_prior,\"%Y%m%d\")\n",
    "    return(starttime)\n",
    "\n",
    "\n",
    "def get_old_page_length(pagetitle, oldrevid):\n",
    "    pwsite = pwb.Site(\"en\", \"wikipedia\")\n",
    "    pwpage = pwb.Page(pwsite, pagetitle)\n",
    "    text = pwpage.getOldVersion(oldid = oldrevid)\n",
    "    return(len(text))\n",
    "\n",
    "\n",
    "def get_old_page_volumes(mwsite,no_missing):\n",
    "    print('obtaining old wikipedia volume information')\n",
    "    no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "    pageinfo=[]\n",
    "    for i in range(len(no_missing)):\n",
    "        pagetitle = no_missing.iloc[i]['titlelist']\n",
    "        updatedate = no_missing.iloc[i]['Wikipedia update period']\n",
    "        starttime = get_six_months_prior(updatedate)\n",
    "        tempdict={'title':pagetitle,'Wikipedia update period':updatedate,'6 months before update':starttime}\n",
    "        try:\n",
    "            oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "            oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "            tempdict['first revision prior to 6 month date'] = oldrevid\n",
    "            tempdict['corresponding length'] = oldpagevolume\n",
    "            pageinfo.append(tempdict)\n",
    "        except:\n",
    "            ## The page did not exist six months prior to the author adding, so page volume prior is 0\n",
    "            tempdict['first revision prior to 6 month date'] = 0\n",
    "            tempdict['corresponding length'] = 0\n",
    "            pageinfo.append(tempdict)               \n",
    "        time.sleep(1)\n",
    "    return(pageinfo)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pulling gene specific infor by Wikipedia titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surfactant_protein_A1\n"
     ]
    }
   ],
   "source": [
    "## Import the urls for the genes\n",
    "gene_wiki_info = read_csv(datapath+'GeneWikiReviewlist.tsv',delimiter='\\t', header=0)\n",
    "no_missing = gene_wiki_info.loc[~gene_wiki_info['Gene Wiki Page'].isna()].copy()\n",
    "no_missing['titlelist'] = [x.replace(\" \",\"_\").replace(\"https://\",\"http://\").replace(\"http://en.wikipedia.org/wiki/\",\"\") for x in no_missing['Gene Wiki Page']]\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(int)\n",
    "no_missing['Wikipedia update period'] = no_missing['Wikipedia update period'].astype(str)\n",
    "print(no_missing.iloc[0]['titlelist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining wikipedia volume information\n",
      "                   title  page_length          last_touched   lastrevid\n",
      "0  Surfactant protein A1        31194  2021-11-25T06:20:36Z  1056213617\n",
      "1  Surfactant protein A2        31602  2021-11-19T17:21:15Z  1056087140\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Get Wikipedia info for gene wiki articles\n",
    "pageinfo,pagefails = get_wiki_volume_info(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining old wikipedia volume information\n",
      "                   title Wikipedia update period 6 months before update  \\\n",
      "0  Surfactant_protein_A1                20120830               20120229   \n",
      "1  Surfactant_protein_A2                20120830               20120229   \n",
      "\n",
      "   first revision prior to 6 month date  corresponding length  \n",
      "0                             462319030                  5903  \n",
      "1                             204784710                    21  \n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Get past Wikipedia info for gene wiki articles\n",
    "pageinfo = get_old_page_volumes(mwsite,no_missing)\n",
    "wikiinfo = pd.DataFrame(pageinfo)\n",
    "print(wikiinfo.head(n=2))\n",
    "\n",
    "wikiinfo.to_csv(exppath+'gene_wiki_vol_info-BEFORE.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462319030\n",
      "5903\n"
     ]
    }
   ],
   "source": [
    "#### Test of functions\n",
    "pagetitle = 'Surfactant_protein_A1'\n",
    "starttime = '20120229'\n",
    "oldrevid = get_revid(mwsite,pagetitle,starttime)\n",
    "print(oldrevid)\n",
    "oldpagevolume = get_old_page_length(pagetitle,oldrevid)\n",
    "print(oldpagevolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining wikipedia pageview information\n",
      "                   title  views granularity   timestamp      access agent\n",
      "0  Surfactant_protein_A1    146     monthly  2017090100  all-access  user\n",
      "1  Surfactant_protein_A1    125     monthly  2017100100  all-access  user\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Get Page views for each Gene Wiki Review wikipedia entry\n",
    "\n",
    "#pages = [\"Cyclin-dependent kinase 1\", \"Reelin\"] ## for unit test\n",
    "\n",
    "pv_params = {'project':'en.wikipedia',\n",
    "             'access':'all-access/',\n",
    "             'agent':'user/',\n",
    "             'granularity':'monthly/',\n",
    "             'start':'20130101',\n",
    "             'end':'20211115'}\n",
    "\n",
    "gene_monthly_pvs,pgfails = get_monthly_pvs(pv_params,useragent, no_missing)\n",
    "print(gene_monthly_pvs.head(n=2))\n",
    "\n",
    "gene_monthly_pvs.to_csv(exppath+'gene_wiki_views.tsv',sep='\\t',header=True)\n",
    "\n",
    "gene_monthly_views = pd.pivot_table(gene_monthly_pvs[['timestamp','title','views']],\n",
    "                                        values='views',index='title',columns='timestamp')\n",
    "gene_pvs = gene_monthly_views.reset_index()\n",
    "gene_pvs.rename(columns={'title':'wikipedia'},inplace=True)\n",
    "#print(gene_pvs)\n",
    "gene_pvs.to_csv(exppath+'gw_pvs.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all statements added for series via SPARQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Wikidata for P179 (part of series) of Q108807010 (Gene Wiki Review Series). Then identify statements that use any member of the query results as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the sparql query to retrieve all Articles in this series\n",
    "\n",
    "def fetch_gwreviews_wd():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?itemLabel \n",
    "    WHERE \n",
    "    {\n",
    "      ?item wdt:P179 wd:Q108807010. \n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } # Helps get the label in your language, if not, then en language\n",
    "    }\n",
    "    \"\"\"\n",
    "    r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "    datadf = pd.DataFrame(data['results']['bindings'])\n",
    "    datadf['uri'] = [x['value'] for x in datadf['item']]\n",
    "    datadf['label'] = [x['value'] for x in datadf['itemLabel']]\n",
    "    datadf['QID'] = [x.replace('http://www.wikidata.org/entity/','') for x in datadf['uri']]\n",
    "    cleandata = datadf[['uri','label','QID']].copy()\n",
    "    return(cleandata)\n",
    "\n",
    "\n",
    "def load_props_to_check(DATAPATH):\n",
    "    propinfo = read_csv(os.path.join(DATAPATH,'propertylist.tsv'),delimiter='\\t',header=0)\n",
    "    return(propinfo)\n",
    "\n",
    "\n",
    "def clean_up_results(wdjson,pid):\n",
    "    tmpdf = pd.DataFrame(wdjson['results']['bindings'])\n",
    "    tmpdf['subjectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['item']]\n",
    "    tmpdf['objectQID'] = [x['value'].replace(\"http://www.wikidata.org/entity/\",\"\") for x in tmpdf['value']]\n",
    "    tmpdf['subject'] = [x['value'] for x in tmpdf['itemLabel']]\n",
    "    tmpdf['object'] = [x['value'] for x in tmpdf['valueLabel']]\n",
    "    tmpdf['predicatePID'] = pid\n",
    "    cleandf = tmpdf[['subjectQID','predicatePID','objectQID','subject','object']].copy()\n",
    "    return(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 26min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run query to retrieve all statements that reference the above articles\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "propinfo = load_props_to_check(DATAPATH)\n",
    "usefulpids = propinfo['Property'].loc[propinfo['PropertyUse']=='main'].unique().tolist()\n",
    "\n",
    "cleandata = fetch_gwreviews_wd()\n",
    "refids = cleandata['QID'].unique().tolist()\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "querybase = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  \"\"\"\n",
    "\n",
    "queryend = \"\"\"    \n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "resultdf = pd.DataFrame(columns = (['subjectQID','predicatePID','objectQID','subject','object']))\n",
    "for refqid in refids:\n",
    "    for pid in usefulpids:\n",
    "        #refqid = 'Q65950306' ## For testing\n",
    "        #pid = 'P1916' ## For testing\n",
    "        refquery = f\"  ?ref ?prop wd:{refqid} .\" \n",
    "        propquery = f\"  ?item wdt:{pid} ?value\"  \n",
    "        query = querybase+refquery+propquery+queryend\n",
    "        try:\n",
    "            r = httprequests.get(url, params = {'format': 'json', 'query': query})\n",
    "            tmpdata = r.json()\n",
    "            if len(tmpdata['results']['bindings']) <= 0:\n",
    "                no_result_flag = True\n",
    "            else:\n",
    "                cleandf = clean_up_results(tmpdata,pid)\n",
    "                resultdf = pd.concat((resultdf,cleandf),ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "resultdf.to_csv('results/wd_statements_added.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a test\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "SELECT DISTiNCT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?ref . \n",
    "  ?ref ?prop wd:Q102060922 . # Replace with specific QID of article as a reference under 'stated in' within a statement\n",
    "  ?item wdt:P2293 ?value  # Specify property of statement (in this example, Genetic Assocation)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "test = r.json()\n",
    "print(pd.DataFrame(test['results']['bindings']))\n",
    "\n",
    "\n",
    "cleantest = clean_up_results(test,'P2293')\n",
    "print(cleantest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
